from scipy import stats
from sklearn.linear_model import LinearRegression
from statsmodels.compat import lzip
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.formula.api as smf
import statsmodels.stats.api as sms
from scipy import stats
from statsmodels.compat import lzip
import statsmodels

%matplotlib inline

#Read in data
df2 =pd.read_csv('/content/auto_imp.csv')

df2

	fuel_type 	wheel_base 	length 	width 	heights 	curb_weight 	engine_size 	bore 	stroke 	comprassion 	horse_power 	peak_rpm 	city_mpg 	highway_mpg 	price
0 	gas 	88.6 	168.8 	64.1 	48.8 	2548 	130 	3.47 	2.68 	9.0 	111 	5000 	21 	27 	13495
1 	gas 	88.6 	168.8 	64.1 	48.8 	2548 	130 	3.47 	2.68 	9.0 	111 	5000 	21 	27 	16500
2 	gas 	94.5 	171.2 	65.5 	52.4 	2823 	152 	2.68 	3.47 	9.0 	154 	5000 	19 	26 	16500
3 	gas 	99.8 	176.6 	66.2 	54.3 	2337 	109 	3.19 	3.40 	10.0 	102 	5500 	24 	30 	13950
4 	gas 	99.4 	176.6 	66.4 	54.3 	2824 	136 	3.19 	3.40 	8.0 	115 	5500 	18 	22 	17450
... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	...
190 	gas 	109.1 	188.8 	68.9 	55.5 	2952 	141 	3.78 	3.15 	9.5 	114 	5400 	23 	28 	16845
191 	gas 	109.1 	188.8 	68.8 	55.5 	3049 	141 	3.78 	3.15 	8.7 	160 	5300 	19 	25 	19045
192 	gas 	109.1 	188.8 	68.9 	55.5 	3012 	173 	3.58 	2.87 	8.8 	134 	5500 	18 	23 	21485
193 	diesel 	109.1 	188.8 	68.9 	55.5 	3217 	145 	3.01 	3.40 	23.0 	106 	4800 	26 	27 	22470
194 	gas 	109.1 	188.8 	68.9 	55.5 	3062 	141 	3.78 	3.15 	9.5 	114 	5400 	19 	25 	22625

195 rows × 15 columns
1. Data
Get dummy variables for fuel_type

df2=pd.get_dummies(df2, columns=['fuel_type'],drop_first=True)

df2.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 195 entries, 0 to 194
Data columns (total 15 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   wheel_base     195 non-null    float64
 1   length         195 non-null    float64
 2   width          195 non-null    float64
 3   heights        195 non-null    float64
 4   curb_weight    195 non-null    int64  
 5   engine_size    195 non-null    int64  
 6   bore           195 non-null    float64
 7   stroke         195 non-null    float64
 8   comprassion    195 non-null    float64
 9   horse_power    195 non-null    int64  
 10  peak_rpm       195 non-null    int64  
 11  city_mpg       195 non-null    int64  
 12  highway_mpg    195 non-null    int64  
 13  price          195 non-null    int64  
 14  fuel_type_gas  195 non-null    uint8  
dtypes: float64(7), int64(7), uint8(1)
memory usage: 21.6 KB

df2.head()

	wheel_base 	length 	width 	heights 	curb_weight 	engine_size 	bore 	stroke 	comprassion 	horse_power 	peak_rpm 	city_mpg 	highway_mpg 	price 	fuel_type_gas
0 	88.6 	168.8 	64.1 	48.8 	2548 	130 	3.47 	2.68 	9.0 	111 	5000 	21 	27 	13495 	1
1 	88.6 	168.8 	64.1 	48.8 	2548 	130 	3.47 	2.68 	9.0 	111 	5000 	21 	27 	16500 	1
2 	94.5 	171.2 	65.5 	52.4 	2823 	152 	2.68 	3.47 	9.0 	154 	5000 	19 	26 	16500 	1
3 	99.8 	176.6 	66.2 	54.3 	2337 	109 	3.19 	3.40 	10.0 	102 	5500 	24 	30 	13950 	1
4 	99.4 	176.6 	66.4 	54.3 	2824 	136 	3.19 	3.40 	8.0 	115 	5500 	18 	22 	17450 	1
1.2 EDA

Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.

Follow the lecture notes for ideas of how to perform EDA on your dataset. For help, here are the steps we talked about:

Suggested Steps in EDA:

Provide descriptions of your sample and features
Check for missing data
Identify the shape of your data
Identify significant correlations

These steps are a guidline. Try different things and share your insights about the dataset (df2).

Don't forget to add "markdown" cells to include your findings or to explain what you are doing
1. Provide descriptions of your sample and features

How many rows and columns?

df2.shape

(195, 15)

It tells us that the dataframe contains 15 columns and 195 rows. In other words: 15 features and 195 observations

What type of data?

df2.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 195 entries, 0 to 194
Data columns (total 15 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   wheel_base     195 non-null    float64
 1   length         195 non-null    float64
 2   width          195 non-null    float64
 3   heights        195 non-null    float64
 4   curb_weight    195 non-null    int64  
 5   engine_size    195 non-null    int64  
 6   bore           195 non-null    float64
 7   stroke         195 non-null    float64
 8   comprassion    195 non-null    float64
 9   horse_power    195 non-null    int64  
 10  peak_rpm       195 non-null    int64  
 11  city_mpg       195 non-null    int64  
 12  highway_mpg    195 non-null    int64  
 13  price          195 non-null    int64  
 14  fuel_type_gas  195 non-null    uint8  
dtypes: float64(7), int64(7), uint8(1)
memory usage: 21.6 KB

In the above scenario, all the columns are of the numeric type with non-Null entries.

Summary Statistics

df2.describe()

	wheel_base 	length 	width 	heights 	curb_weight 	engine_size 	bore 	stroke 	comprassion 	horse_power 	peak_rpm 	city_mpg 	highway_mpg 	price 	fuel_type_gas
count 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000 	195.000000
mean 	98.896410 	174.256923 	65.886154 	53.861538 	2559.000000 	127.938462 	3.329385 	3.250308 	10.194974 	103.271795 	5099.487179 	25.374359 	30.841026 	13248.015385 	0.897436
std 	6.132038 	12.476443 	2.132484 	2.396778 	524.715799 	41.433916 	0.271866 	0.314115 	4.062109 	37.869730 	468.271381 	6.401382 	6.829315 	8056.330093 	0.304170
min 	86.600000 	141.100000 	60.300000 	47.800000 	1488.000000 	61.000000 	2.540000 	2.070000 	7.000000 	48.000000 	4150.000000 	13.000000 	16.000000 	5118.000000 	0.000000
25% 	94.500000 	166.300000 	64.050000 	52.000000 	2145.000000 	98.000000 	3.150000 	3.110000 	8.500000 	70.000000 	4800.000000 	19.500000 	25.000000 	7756.500000 	1.000000
50% 	97.000000 	173.200000 	65.400000 	54.100000 	2414.000000 	120.000000 	3.310000 	3.290000 	9.000000 	95.000000 	5100.000000 	25.000000 	30.000000 	10245.000000 	1.000000
75% 	102.400000 	184.050000 	66.900000 	55.650000 	2943.500000 	145.500000 	3.590000 	3.410000 	9.400000 	116.000000 	5500.000000 	30.000000 	35.000000 	16509.000000 	1.000000
max 	120.900000 	208.100000 	72.000000 	59.800000 	4066.000000 	326.000000 	3.940000 	4.170000 	23.000000 	262.000000 	6600.000000 	49.000000 	54.000000 	45400.000000 	1.000000

We can see the count of each column along with their quartiles(25%,50%,75%), mean value, standard deviation, minimum and maximum values.

data = df2.drop_duplicates()
data

	wheel_base 	length 	width 	heights 	curb_weight 	engine_size 	bore 	stroke 	comprassion 	horse_power 	peak_rpm 	city_mpg 	highway_mpg 	price 	fuel_type_gas
0 	88.6 	168.8 	64.1 	48.8 	2548 	130 	3.47 	2.68 	9.0 	111 	5000 	21 	27 	13495 	1
1 	88.6 	168.8 	64.1 	48.8 	2548 	130 	3.47 	2.68 	9.0 	111 	5000 	21 	27 	16500 	1
2 	94.5 	171.2 	65.5 	52.4 	2823 	152 	2.68 	3.47 	9.0 	154 	5000 	19 	26 	16500 	1
3 	99.8 	176.6 	66.2 	54.3 	2337 	109 	3.19 	3.40 	10.0 	102 	5500 	24 	30 	13950 	1
4 	99.4 	176.6 	66.4 	54.3 	2824 	136 	3.19 	3.40 	8.0 	115 	5500 	18 	22 	17450 	1
... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	...
190 	109.1 	188.8 	68.9 	55.5 	2952 	141 	3.78 	3.15 	9.5 	114 	5400 	23 	28 	16845 	1
191 	109.1 	188.8 	68.8 	55.5 	3049 	141 	3.78 	3.15 	8.7 	160 	5300 	19 	25 	19045 	1
192 	109.1 	188.8 	68.9 	55.5 	3012 	173 	3.58 	2.87 	8.8 	134 	5500 	18 	23 	21485 	1
193 	109.1 	188.8 	68.9 	55.5 	3217 	145 	3.01 	3.40 	23.0 	106 	4800 	26 	27 	22470 	0
194 	109.1 	188.8 	68.9 	55.5 	3062 	141 	3.78 	3.15 	9.5 	114 	5400 	19 	25 	22625 	1

192 rows × 15 columns

In the case mentioned above, we are eliminating all the duplicates that are present in our dataset.

df2.columns

Index(['wheel_base', 'length', 'width', 'heights', 'curb_weight',
       'engine_size', 'bore', 'stroke', 'comprassion', 'horse_power',
       'peak_rpm', 'city_mpg', 'highway_mpg', 'price', 'fuel_type_gas'],
      dtype='object')

The various features of automobiles that are present in our dataset can be seen in the code above.

Is our data balanced?

df2.value_counts()

wheel_base  length  width  heights  curb_weight  engine_size  bore  stroke  comprassion  horse_power  peak_rpm  city_mpg  highway_mpg  price  fuel_type_gas
93.7        157.3   63.8   50.8     2128         98           3.03  3.39    7.6          102          5500      24        30           7957   1                2
96.3        172.4   65.4   51.6     2403         110          3.17  3.46    7.5          116          5500      23        30           9279   1                2
93.7        157.3   63.8   50.6     1967         90           2.97  3.23    9.4          68           5500      31        38           6229   1                2
86.6        144.6   63.9   50.8     1713         92           2.91  3.41    9.6          58           4800      49        54           6479   1                1
100.4       181.7   66.5   55.1     3095         181          3.43  3.27    9.0          152          5200      17        22           13499  1                1
                                                                                                                                                              ..
95.7        169.7   63.6   59.1     2290         92           3.05  3.03    9.0          62           4800      27        32           7898   1                1
                                    3110         92           3.05  3.03    9.0          62           4800      27        32           8778   1                1
95.9        173.2   66.3   50.2     2811         156          3.60  3.90    7.0          145          5000      19        24           12964  1                1
                                    2818         156          3.59  3.86    7.0          145          5000      19        24           12764  1                1
120.9       208.1   71.7   56.7     3900         308          3.80  3.35    8.0          184          4500      14        16           40960  1                1
Length: 192, dtype: int64

No, the data we have are not balanced. We can see that each feature of automobile has a different number of counts.
2. Check for missing data

Checking for null/missing values

df2.isnull().sum()

wheel_base       0
length           0
width            0
heights          0
curb_weight      0
engine_size      0
bore             0
stroke           0
comprassion      0
horse_power      0
peak_rpm         0
city_mpg         0
highway_mpg      0
price            0
fuel_type_gas    0
dtype: int64

According to the above observation there are no null/missing values in any of the rows.
3. Identify the shape of your data

# importing packages
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
 
 
sns.countplot(x='fuel_type_gas', data=df2)
plt.show()

In the above countplot, the observation is made that the gas fuel type is the highest and diesel fuel type is the lowest.

sns.scatterplot(x="city_mpg", y="highway_mpg", data=df2)

# Set plot title and axis labels
plt.title("City_mpg vs Highway_mpg")
plt.xlabel("city_mpg")
plt.ylabel("highway_mpg")

# Show plot
plt.show()

The first scatter plot shows the relationship between city_mpg (x-axis) and highway_mpg (y-axis), and reveals a strong positive correlation between the two variables.

sns.scatterplot(x="fuel_type_gas", y="comprassion", data=df2)

# Set plot title and axis labels
plt.title("fuel_type_gas vs comprassion")
plt.xlabel("fuel_type_gas")
plt.ylabel("comprassion")

# Show plot
plt.show()

The second scatter plot shows the relationship between city fuel_type_gas (x-axis) and comprassion(y-axis), and reveals a strong negative correlation between the two variables.:

#compare all the variables
sns.pairplot(df2,height=2)

# Show plot
plt.show()

This code creates a pair plot for all variables in the auto dataset. The diagonal of the plot shows a histogram for each variable, while the off-diagonal plots show scatter plots for each pair of variables. This plot can help identify any potential correlations between variables in the dataset.

#histograms
fig, axes = plt.subplots(2, 2, figsize=(10,10))
 
axes[0,0].set_title("curb_weight")
axes[0,0].hist(df2['curb_weight'], bins=7)
 
axes[0,1].set_title("engine_size")
axes[0,1].hist(df2['engine_size'], bins=5);
 
axes[1,0].set_title("city_mpg")
axes[1,0].hist(df2['city_mpg'], bins=6);
 
axes[1,1].set_title("highway_mpg")
axes[1,1].hist(df2['highway_mpg'], bins=6);

A histogram is a graphical representation of a frequency distribution, showing the distribution of numerical data.In this case, we're making a histogram to show the frequency distribution of the data points for the relevant variable. We used the automobile's curb_weight, engine_size, city_mpg, and highway_mpg as our variables in the observation above.
4. Identify significant correlations

data.corr(method='pearson')

	wheel_base 	length 	width 	heights 	curb_weight 	engine_size 	bore 	stroke 	comprassion 	horse_power 	peak_rpm 	city_mpg 	highway_mpg 	price 	fuel_type_gas
wheel_base 	1.000000 	0.879105 	0.817488 	0.587941 	0.780877 	0.566168 	0.492531 	0.175939 	0.244148 	0.374556 	-0.346298 	-0.500366 	-0.566584 	0.582573 	-0.301448
length 	0.879105 	1.000000 	0.856740 	0.487030 	0.880830 	0.684930 	0.602827 	0.123326 	0.155573 	0.584421 	-0.272354 	-0.693719 	-0.721995 	0.693423 	-0.207764
width 	0.817488 	0.856740 	1.000000 	0.306296 	0.866049 	0.738204 	0.538645 	0.190494 	0.187317 	0.617003 	-0.244505 	-0.649102 	-0.693235 	0.752554 	-0.242985
heights 	0.587941 	0.487030 	0.306296 	1.000000 	0.297743 	0.018081 	0.173095 	-0.050366 	0.255740 	-0.090866 	-0.252383 	-0.100798 	-0.147289 	0.127201 	-0.276170
curb_weight 	0.780877 	0.880830 	0.866049 	0.297743 	1.000000 	0.856419 	0.641327 	0.176616 	0.151589 	0.761182 	-0.271890 	-0.774549 	-0.813980 	0.834570 	-0.216870
engine_size 	0.566168 	0.684930 	0.738204 	0.018081 	0.856419 	1.000000 	0.578620 	0.216128 	0.019877 	0.844444 	-0.211808 	-0.712730 	-0.732986 	0.888095 	-0.060131
bore 	0.492531 	0.602827 	0.538645 	0.173095 	0.641327 	0.578620 	1.000000 	-0.063262 	-0.004074 	0.569326 	-0.268043 	-0.594894 	-0.600856 	0.542279 	-0.051424
stroke 	0.175939 	0.123326 	0.190494 	-0.050366 	0.176616 	0.216128 	-0.063262 	1.000000 	0.204424 	0.099086 	-0.073537 	-0.025695 	-0.035584 	0.097242 	-0.256284
comprassion 	0.244148 	0.155573 	0.187317 	0.255740 	0.151589 	0.019877 	-0.004074 	0.204424 	1.000000 	-0.215372 	-0.441435 	0.331922 	0.269615 	0.065087 	-0.985917
horse_power 	0.374556 	0.584421 	0.617003 	-0.090866 	0.761182 	0.844444 	0.569326 	0.099086 	-0.215372 	1.000000 	0.109376 	-0.833454 	-0.812108 	0.812657 	0.170148
peak_rpm 	-0.346298 	-0.272354 	-0.244505 	-0.252383 	-0.271890 	-0.211808 	-0.268043 	-0.073537 	-0.441435 	0.109376 	1.000000 	-0.071393 	-0.020721 	-0.096199 	0.479611
city_mpg 	-0.500366 	-0.693719 	-0.649102 	-0.100798 	-0.774549 	-0.712730 	-0.594894 	-0.025695 	0.331922 	-0.833454 	-0.071393 	1.000000 	0.972459 	-0.704771 	-0.262195
highway_mpg 	-0.566584 	-0.721995 	-0.693235 	-0.147289 	-0.813980 	-0.732986 	-0.600856 	-0.035584 	0.269615 	-0.812108 	-0.020721 	0.972459 	1.000000 	-0.716345 	-0.196162
price 	0.582573 	0.693423 	0.752554 	0.127201 	0.834570 	0.888095 	0.542279 	0.097242 	0.065087 	0.812657 	-0.096199 	-0.704771 	-0.716345 	1.000000 	-0.105893
fuel_type_gas 	-0.301448 	-0.207764 	-0.242985 	-0.276170 	-0.216870 	-0.060131 	-0.051424 	-0.256284 	-0.985917 	0.170148 	0.479611 	-0.262195 	-0.196162 	-0.105893 	1.000000

sns.heatmap(df2.corr(method='pearson'),annot = True);
 
plt.show()

From the heatmap, we can see that some of the strongest positive correlations are between:

    city mpg and highway mpg(0.97)
    Length and wheel base (0.88)
    Curb weight and engine size (0.86)

On the other hand, there are some strong negative correlations as well, such as:

    fuel type gas and comprassion (-0.99)
    horse power and city mpg (-0.83)
    highway mpg and engine size (-0.73)

2. Multiple Regression Analysis

1. Create a model that uses all the variables and call it model1. The dependent variable is price, the independent variables are all the rest. Print out a summary of the model (coefficents, stanrard errors, confidence intervals and other metrics shown in class and answer the quesions based on your output.

import statsmodels.api as sm

# create X and y
X = df2.drop('price', axis=1)
y = df2['price']

# add constant to X
X = sm.add_constant(X)

# fit the model
model1 = sm.OLS(y, X).fit()

# print the summary
print(model1.summary())

                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R-squared:                       0.860
Model:                            OLS   Adj. R-squared:                  0.849
Method:                 Least Squares   F-statistic:                     78.89
Date:                Sun, 30 Apr 2023   Prob (F-statistic):           5.84e-69
Time:                        01:05:28   Log-Likelihood:                -1838.5
No. Observations:                 195   AIC:                             3707.
Df Residuals:                     180   BIC:                             3756.
Df Model:                          14                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const          -4.45e+04   1.84e+04     -2.419      0.017   -8.08e+04   -8194.301
wheel_base       39.5305    103.549      0.382      0.703    -164.796     243.857
length          -60.6333     58.500     -1.036      0.301    -176.068      54.801
width           603.6414    254.539      2.372      0.019     101.377    1105.906
heights         329.5669    140.947      2.338      0.020      51.446     607.688
curb_weight       1.1798      1.738      0.679      0.498      -2.249       4.609
engine_size     138.4537     16.111      8.594      0.000     106.662     170.245
bore          -1208.4137   1206.683     -1.001      0.318   -3589.479    1172.651
stroke        -3706.0531    874.513     -4.238      0.000   -5431.669   -1980.437
comprassion    -617.1497    446.452     -1.382      0.169   -1498.103     263.804
horse_power      34.6328     18.049      1.919      0.057      -0.982      70.248
peak_rpm          2.5517      0.709      3.599      0.000       1.153       3.951
city_mpg       -288.2868    180.791     -1.595      0.113    -645.030      68.456
highway_mpg     316.6334    163.540      1.936      0.054      -6.069     639.336
fuel_type_gas -1.173e+04   6002.268     -1.955      0.052   -2.36e+04     110.854
==============================================================================
Omnibus:                       18.136   Durbin-Watson:                   0.978
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.211
Skew:                           0.240   Prob(JB):                     1.03e-12
Kurtosis:                       5.562   Cond. No.                     4.77e+05
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 4.77e+05. This might indicate that there are
strong multicollinearity or other numerical problems.

# variance of the model1
model1.mse_resid

9802534.782392945

Answer the questions based on the output

    How do you interpret the intercept?

        The intercept is the estimated value of the dependent variable (price) when all the independent variables are equal to zero. In this model, the intercept is -4.45e+04. This means that if all the independent variables are zero, the price of the automobile is estimated to be negative $44,500, which is not a meaningful interpretation in this case.

    How many variables are statistically significant?

        There are several variables that have statistically significant coefficients. The variables with p-values less than 0.05 (assuming a 95% confidence level) are: width, heights, engine_size, stroke and peak_rpm.

    What is the variance of the model?

        9802534.782392945 this is the variance of model1 found using mean squared error of the residuals.

    What is the coefficeint of determination and how do you interpret it?

        The coefficient of determination, or R-squared value, is 0.86. This means that approximately 86% of the variation in the dependent variable (price) can be explained by the independent variables in the model. In other words, the model is a good fit and can explain a large proportion of the variance in the dependent variable.

    What is the F-statistics used for? How do you interpret it for this model?

        The F-statistic is used to test the overall significance of the regression model. The null hypothesis for the F-test is that all of the coefficients in the model are equal to zero, meaning that none of the independent variables have a significant effect on the dependent variable. The F-statistic for this model is 78.89, with a corresponding p-value of 5.84e-69. This p-value is extremely small, indicating that we can reject the null hypothesis and conclude that at least one of the independent variables has a significant effect on the dependent variable

2. Dropp all the variables that are not statistically significant at least at 90% confidence level. Run another regression model with price as the dependent variable and the rest of the variables as the independent variabls. Call it model2. Print a summary of the results and answer the questions bellow.

## your code starts here
import statsmodels.api as sm

# Dropping non-significant variables from the previous model
X = df2[['width','heights','engine_size','stroke','horse_power','peak_rpm','highway_mpg','fuel_type_gas']]
y = df2['price']

# Adding constant to the model
X = sm.add_constant(X)

# Fitting the regression model
model2 = sm.OLS(y, X).fit()

# Printing the summary of the results
print(model2.summary())

                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R-squared:                       0.854
Model:                            OLS   Adj. R-squared:                  0.848
Method:                 Least Squares   F-statistic:                     136.5
Date:                Sun, 30 Apr 2023   Prob (F-statistic):           1.29e-73
Time:                        01:05:29   Log-Likelihood:                -1842.2
No. Observations:                 195   AIC:                             3702.
Df Residuals:                     186   BIC:                             3732.
Df Model:                           8                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const         -6.156e+04   1.49e+04     -4.138      0.000   -9.09e+04   -3.22e+04
width           566.4440    196.395      2.884      0.004     178.995     953.893
heights         289.5923    113.100      2.561      0.011      66.470     512.715
engine_size     131.2899     13.724      9.566      0.000     104.215     158.365
stroke        -2942.0083    775.472     -3.794      0.000   -4471.859   -1412.158
horse_power      43.1639     15.720      2.746      0.007      12.152      74.176
peak_rpm          2.3532      0.639      3.683      0.000       1.093       3.614
highway_mpg      39.9588     69.437      0.575      0.566     -97.027     176.944
fuel_type_gas -3384.0156    998.300     -3.390      0.001   -5353.463   -1414.568
==============================================================================
Omnibus:                       18.233   Durbin-Watson:                   0.944
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               61.116
Skew:                           0.177   Prob(JB):                     5.36e-14
Kurtosis:                       5.720   Cond. No.                     3.39e+05
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.39e+05. This might indicate that there are
strong multicollinearity or other numerical problems.

#variance of model2
model2.mse_resid

9852556.110539043

Answer the questions based on the output

    How do you interpret the intercept?

        The intercept (-6.156e+04) represents the predicted value of the dependent variable (price) when all independent variables (width, heights, engine_size, stroke, horse_power, peak_rpm, highway_mpg, and fuel_type_gas) are equal to zero. However, since it is unlikely for all independent variables to be zero, the intercept is mostly used to center the data and to help with the interpretation of the regression coefficients.

    How many variables are statistically significant?

        In this model, all eight independent variables (width, heights, engine_size, stroke, horse_power, peak_rpm,highway_mpg and fuel_type_gas) have p-values less than 0.05, which indicates that they are statistically significant at the 95% confidence level.

    What is the variance of the model?

        9852556.110539043 this is the variance of model2 found using mean squared error of the residuals.

    What is the coefficeint of determination and how do you interpret it? What is the Adjusted R-squared and compare it to the model1's value.

        The coefficient of determination (R-squared) is 0.854, which means that about 85.4% of the variability in the dependent variable (price) can be explained by the independent variables in the model. This is a relatively high value, indicating that the model is a good fit for the data. The adjusted R-squared is 0.848, which is slightly lower than the R-squared value in model 1. This indicates that the variables dropped from the model may have had some influence on the model's predictive power. However, since Model2 includes more independent variables, it may be considered a better fit for the data as it accounts for more factors that can potentially affect the outcome variable.

    What is the F-statistics used for? How do you interpret it for this model?

        The F-statistic is used to test the overall significance of the regression model. Specifically, it tests whether at least one of the independent variables has a non-zero coefficient (i.e., whether the regression model is better than a model with only an intercept). In this model, the F-statistic is 136.5 with a corresponding p-value of 1.29e-73. This indicates that the model as a whole is statistically significant and better than a model with only an intercept.

3. Compare the two models with ANOVA. What are your null and alternative hypothesis? What is your conclusion?

##your code goes herefrom statsmodels.stats.anova import anova_lm

anova_results = anova_lm(model1, model2)
print(anova_results)

   df_resid           ssr  df_diff       ss_diff        F  Pr(>F)
0     180.0  1.764456e+09      0.0           NaN      NaN     NaN
1     186.0  1.832575e+09     -6.0 -6.811918e+07  1.15231     NaN

The ANOVA table is used to compare the two models and test whether adding the additional variables in Model 2 has led to a statistically significant improvement in the fit of the model compared to Model 1.

The null hypothesis for ANOVA is that the two models do not differ significantly, and the alternative hypothesis is that they do.

Comparing the two models using ANOVA, we can see that the F-statistic is 1.15231 and the p-value is NaN, indicating that the difference between the models is not statistically significant at the 5% significance level. Therefore, we fail to reject the null hypothesis and conclude that there is no significant difference between the two models..

4. Checking the assumptions on the model you chose based on ANOVA:

-What are the assumptions?

-Do they hold? How do you test/check?

The assumptions of linear regression are:

    Linearity: The relationship between the independent and dependent variables is linear.

    Independence: The residuals should be independent of each other.

    Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables.

    Normality: The residuals should be normally distributed.

    No multicollinearity: The independent variables should not be highly correlated with each other.

We can check the assumptions using different techniques:

    Linearity: We can plot the dependent variable against each independent variable and check for linearity.

    Independence: We can plot the residuals against the predicted values and check for any patterns.

    Homoscedasticity: We can plot the residuals against the predicted values and check for constant variance.

    Normality: We can plot a histogram of the residuals and check for normality. We can also use statistical tests like the Shapiro-Wilk test or the Anderson-Darling test.

    No multicollinearity: We can check the correlation matrix between the independent variables and look for high correlations.

import statsmodels.formula.api as smf
import statsmodels.stats.api as sms
from scipy import stats
from statsmodels.compat import lzip
import statsmodels
statsmodels.stats.stattools.durbin_watson(model2.resid)

0.9437520872376993

Check for normality

Assumption: Normality of residuals.

##Normality test
import scipy.stats as stats

residuals = model2.resid
fig, ax = plt.subplots(figsize=(6,6))
stats.probplot(residuals, plot=ax, fit=True)
plt.title("Model2 Residuals Q-Q Plot")
plt.show()


from scipy.stats import normaltest

_, p_value = normaltest(residuals)
print(f"p-value for normality test: {p_value:.4f}")

p-value for normality test: 0.0001

Since the p-value for the normality test is less than 0.05, we reject the null hypothesis that the residuals are normally distributed. This means that the residuals are not normally distributed and violate the assumption of normality.
Check for constant variance

#Constant variance test
import matplotlib.pyplot as plt
import seaborn as sns

# Get fitted values and residuals
y_hat = model2.fittedvalues
resid = model2.resid

# Create scatterplot of residuals vs fitted values
sns.scatterplot(x=y_hat, y=resid)
plt.xlabel("Fitted values")
plt.ylabel("Residuals")
plt.title("Residuals vs Fitted Values")

# Add a horizontal line at y=0
plt.axhline(y=0, color='r', linestyle='-')
plt.show()

Based on the plot, it seems like the variance of the residuals is not constant across all levels of the predicted values. Therefore, the assumption of homoscedasticity (i.e. constant variance) may not hold for this model.
Plots to check linearity assumption:

Partial regression plot: This plot shows the relationship between the response variable and one predictor variable while controlling for the other predictor variables. If the relationship is linear, the points should cluster around a straight line..

fig = plt.figure(figsize=(12,8))
fig = sm.graphics.plot_partregress_grid(model2, fig=fig)
plt.show()

eval_env: 1
eval_env: 1
eval_env: 1
eval_env: 1
eval_env: 1
eval_env: 1
eval_env: 1
eval_env: 1
eval_env: 1

From the partial regression plot, it appears that there is a linear relationship between the target variable (price) and each of the predictor variables (width,heights,engine_size,stroke,peak_rpm) when holding the other predictors constant. The plots show a fairly random scatter of points around the regression line, indicating that the linearity assumption is reasonable.

5. Is there Multicollinearity in your data?

Calculate VIF for both the full model and the reduce model. What do you notice?

Here's how we can calculate VIF values for the full model:

# Change array to dataframe. For each X, calculate VIF and save in dataframe. Anything above 10 will suggest multicollinearity
# if you did not change your data into matrix format, you may not need to make any changes. 
from statsmodels.stats.outliers_influence import variance_inflation_factor

# create a DataFrame of the independent variables
X = df2[['fuel_type_gas', 'wheel_base', 'length', 'width', 'heights', 'curb_weight', 'engine_size', 'bore', 'stroke', 'comprassion',
       'horse_power', 'peak_rpm', 'city_mpg', 'highway_mpg', 'price']]

# calculate VIF for each variable
vif = pd.DataFrame()
vif["variables"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# print the results
print(vif)

        variables          VIF
0   fuel_type_gas   523.741129
1      wheel_base  2076.004545
2          length  2024.409569
3           width  3033.401631
4         heights  1033.969461
5     curb_weight   410.013648
6     engine_size   131.194863
7            bore   292.334483
8          stroke   164.196246
9     comprassion   399.315487
10    horse_power    78.830897
11       peak_rpm   278.824977
12       city_mpg   451.434648
13    highway_mpg   536.037917
14          price    25.697641

And here's how we can calculate VIF values for the reduced model:

from statsmodels.stats.outliers_influence import variance_inflation_factor

# create a DataFrame of the independent variables
X = df2[[ 'width','heights','engine_size','stroke','horse_power','peak_rpm','highway_mpg','fuel_type_gas']]

# calculate VIF for each variable
vif = pd.DataFrame()
vif["variables"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# print the results
print(vif)

       variables          VIF
0          width  1228.162140
1        heights   667.767007
2    engine_size    65.944179
3         stroke   124.322109
4    horse_power    57.618941
5       peak_rpm   203.937190
6    highway_mpg    56.405862
7  fuel_type_gas    14.304279

Based on the VIF values, there seems to be a high degree of multicollinearity in the full model as many variables have VIF values greater than 10, indicating that they are highly correlated with other predictors in the model. On the other hand, the reduced model appears to have lower VIF values, suggesting a lower degree of multicollinearity.
